{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutations\n",
    "\n",
    "This notebook demonstrates the effect of changing different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model\n",
    "- vqgan - ...\n",
    "\n",
    "perceptor\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "reencode each frame\n",
    "\n",
    "##############\n",
    "\n",
    "# image_model fixed\n",
    "\n",
    "animation\n",
    "preanimation\n",
    "camera lock\n",
    "\n",
    "cutouts\n",
    "\n",
    "cutpow\n",
    "\n",
    "stabilization modes\n",
    "\n",
    "border mode\n",
    "\n",
    "sampling mode\n",
    "\n",
    "infill mode\n",
    "\n",
    "#############################\n",
    "\n",
    "palettes\n",
    "\n",
    "palette size\n",
    "\n",
    "smoothing\n",
    "\n",
    "gamma\n",
    "\n",
    "hdr weight\n",
    "\n",
    "palette normalization\n",
    "\n",
    "lock palette\n",
    "\n",
    "target palette\n",
    "\n",
    "+/- stabilization weights, modes, etc.\n",
    "\n",
    "#############################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "# animations for limited palette widget\n",
    "# - https://pytti-tools.github.io/pytti-book/widget_understanding_limited_palette.html#widget\n",
    "\n",
    "from pittybook_utils import (\n",
    "    ExperimentMatrix\n",
    ")\n",
    "\n",
    "exp_limited_palette = ExperimentMatrix(\n",
    "    variant = dict(\n",
    "        palettes=(10,30,70),\n",
    "        palette_size=(3,7,15),\n",
    "        #cutouts=(10,50,100),\n",
    "        #cut_pow=(0.5,1,1.5,2),\n",
    "        gamma=(0, 0.1, 1),\n",
    "        hdr_weight=(0, 0.1, 1),\n",
    "        smoothing_weight=(0, 0.1, 1),\n",
    "        #lock_palette=(True,False),\n",
    "        palette_normalization_weight=(0, 0.1, 1),\n",
    "    ),\n",
    "    invariant = dict(\n",
    "        lock_palette=False,\n",
    "        cutouts=60,\n",
    "        cut_pow=1,\n",
    "        allow_overwrite=False,\n",
    "        pixel_size=1,\n",
    "        height=128,\n",
    "        width=256,\n",
    "        #file_namespace=\"permutations_limited_palette_2D\",\n",
    "        scenes=\"fractal crystals | colorful recursions || swirling curves | ethereal neon glow \",\n",
    "        scene_suffix=\" | text:-1:-.9 | watermark:-1:-.9\",\n",
    "        image_model=\"Limited Palette\",\n",
    "        steps_per_frame=50,\n",
    "        steps_per_scene=1000,\n",
    "        interpolation_steps=500,\n",
    "        animation_mode=\"2D\",\n",
    "        translate_y=-1,\n",
    "        zoom_x_2d=3,\n",
    "        zoom_y_2d=3,\n",
    "        seed=12345,\n",
    "    ),\n",
    "    # variable imputation doesn't seem to work in the overrides\n",
    "    mapped = {\n",
    "        'steps_per_frame':('pre_animation_steps', 'save_every'),\n",
    "        'steps_per_scene':('display_every',),\n",
    "    },\n",
    "    #conditional = {'gradient_accumulation_steps': lambda kws: 1 if kws['cutouts'] < 100 else 4}\n",
    "    conditional = {'file_namespace': \n",
    "                    lambda kws: '_'.join(\n",
    "                        [\"permutations_limited_palette_2D\"]+[\n",
    "                            f\"{k}-{v}\" for k,v in kws.items() if k in ('palettes','palette_size','gamma','hdr_weight','smoothing_weight','palette_normalization_weight')]\n",
    "                            )},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall pillow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PIL\n",
    "#PIL.__version__ # 7.2.0\n",
    "#!pip install --upgrade pillow\n",
    "#!pip install --upgrade numpy\n",
    "#!pip install --upgrade scipy\n",
    "# mmc 0.1.0 requires Pillow<8.0.0,>=7.1.2,  \n",
    "# ... I swear I thought I resolved this already, didn't I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dmarx/Multi-Modal-Comparators\n",
    "%cd 'Multi-Modal-Comparators'\n",
    "!pip install poetry\n",
    "!poetry build\n",
    "!pip install dist/mmc*.whl\n",
    "\n",
    "# optional final step:\n",
    "#poe napm_installs\n",
    "!python src/mmc/napm_installs/__init__.py\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "from loguru import logger\n",
    "from pittybook_utils import (\n",
    "    ExperimentMatrix\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "def get_perceptor_ids(in_str):\n",
    "    return re.findall(r\"id:'(.+?)'\", in_str)\n",
    "\n",
    "def fmt_perceptor_string(in_str):\n",
    "    return '_'.join(\n",
    "        [\n",
    "            p.replace('/','') \n",
    "            for p in get_perceptor_ids(in_str)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "exp_vqgan_base_perceptors = ExperimentMatrix(\n",
    "    variant = {\n",
    "        'vqgan_model':(\n",
    "            #'imagenet',\n",
    "            'coco',\n",
    "            'wikiart',\n",
    "            'openimages',\n",
    "            'sflckr',\n",
    "        ),\n",
    "        '+mmc_models':(\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/32'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/16'}]\",\n",
    "            #\"[{architecture:'clip',publisher:'openai',id:'ViT-L/14'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN50'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN101'}]\",\n",
    "            #\"[{architecture:'clip',publisher:'openai',id:'RN50x64'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN50x4'}]\",\n",
    "            #\"[{architecture:'clip',publisher:'openai',id:'RN50x16'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/32'},{architecture:'clip',publisher:'openai',id:'RN50'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/32'},{architecture:'clip',publisher:'openai',id:'ViT-B/16'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN50'},{architecture:'clip',publisher:'openai',id:'RN101'}]\",\n",
    "        ),\n",
    "    },\n",
    "    invariant = {\n",
    "        #'init_image':\"https://www.seattle.gov/images//images/Departments/ParksAndRecreation/Parks/GHI/GasWorksPark3.jpg\",\n",
    "        'init_image':\"/home/dmarx/proj/pytti-book/GasWorksPark3.jpg\",\n",
    "        'direct_stabilization_weight':0.3,\n",
    "        'cutouts':60,\n",
    "        'cut_pow':1,\n",
    "        #'reencode_each_frame':True,\n",
    "        'reencode_each_frame':False,\n",
    "        'reset_lr_each_frame':True,\n",
    "        'allow_overwrite':False,\n",
    "        'pixel_size':1,\n",
    "        'height':128,\n",
    "        'width':256,\n",
    "        'scenes':'\"a photograph of a bright and beautiful spring day, by Trey Ratcliff || a painting of a cold wintery landscape, by Rembrandt \"',\n",
    "        'scene_suffix':'\" | text:-1:-.9 | watermark:-1:-.9\"',\n",
    "        'image_model':\"VQGAN\",\n",
    "        '+use_mmc':True,\n",
    "        'steps_per_frame':50,\n",
    "        'steps_per_scene':1000,\n",
    "        'interpolation_steps':500,\n",
    "        'animation_mode':\"2D\",\n",
    "        #'translate_y':-1,\n",
    "        'translate_x':-1,\n",
    "        'zoom_x_2d':3,\n",
    "        'zoom_y_2d':3,\n",
    "        'seed':12345,\n",
    "    },\n",
    "    # variable imputation doesn't seem to work in the overrides\n",
    "    mapped = {\n",
    "        'steps_per_frame':('pre_animation_steps', 'save_every'),\n",
    "        'steps_per_scene':('display_every',),\n",
    "    },\n",
    "    #conditional = {'gradient_accumulation_steps': lambda kws: 1 if kws['cutouts'] < 100 else 4}\n",
    "    conditional = {'file_namespace':\n",
    "        lambda kws: f\"exp_vqgan_base_perceptors__{kws['vqgan_model']}_{fmt_perceptor_string(kws['+mmc_models'])}\"},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "from loguru import logger\n",
    "from pittybook_utils import (\n",
    "    ExperimentMatrix\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "def get_perceptor_ids(in_str):\n",
    "    return re.findall(r\"id:'(.+?)'\", in_str)\n",
    "\n",
    "def fmt_perceptor_string(in_str):\n",
    "    return '_'.join(\n",
    "        [\n",
    "            p.replace('/','') \n",
    "            for p in get_perceptor_ids(in_str)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "exp_vqgan_base_perceptors_2 = ExperimentMatrix(\n",
    "# These need to be redone because they were blocked by errors\n",
    "variant = {\n",
    "        'vqgan_model':(\n",
    "            'imagenet',\n",
    "        ),\n",
    "        '+mmc_models':(\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/32'},{architecture:'clip',publisher:'openai',id:'RN50'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/32'},{architecture:'clip',publisher:'openai',id:'ViT-B/16'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN50'},{architecture:'clip',publisher:'openai',id:'RN101'}]\",\n",
    "        ),\n",
    "},\n",
    "invariant = {\n",
    "        #'init_image':\"https://www.seattle.gov/images//images/Departments/ParksAndRecreation/Parks/GHI/GasWorksPark3.jpg\",\n",
    "        'init_image':\"/home/dmarx/proj/pytti-book/GasWorksPark3.jpg\",\n",
    "        'direct_stabilization_weight':0.3,\n",
    "        'cutouts':60,\n",
    "        'cut_pow':1,\n",
    "        #'reencode_each_frame':True,\n",
    "        'reencode_each_frame':False,\n",
    "        'reset_lr_each_frame':True,\n",
    "        'allow_overwrite':False,\n",
    "        'pixel_size':1,\n",
    "        'height':128,\n",
    "        'width':256,\n",
    "        'scenes':'\"a photograph of a bright and beautiful spring day, by Trey Ratcliff || a painting of a cold wintery landscape, by Rembrandt \"',\n",
    "        'scene_suffix':'\" | text:-1:-.9 | watermark:-1:-.9\"',\n",
    "        'image_model':\"VQGAN\",\n",
    "        '+use_mmc':True,\n",
    "        'steps_per_frame':50,\n",
    "        'steps_per_scene':1000,\n",
    "        'interpolation_steps':500,\n",
    "        'animation_mode':\"2D\",\n",
    "        #'translate_y':-1,\n",
    "        'translate_x':-1,\n",
    "        'zoom_x_2d':3,\n",
    "        'zoom_y_2d':3,\n",
    "        'seed':12345,\n",
    "    },\n",
    "    # variable imputation doesn't seem to work in the overrides\n",
    "    mapped = {\n",
    "        'steps_per_frame':('pre_animation_steps', 'save_every'),\n",
    "        'steps_per_scene':('display_every',),\n",
    "    },\n",
    "    #conditional = {'gradient_accumulation_steps': lambda kws: 1 if kws['cutouts'] < 100 else 4}\n",
    "    conditional = {'file_namespace':\n",
    "        lambda kws: f\"exp_vqgan_base_perceptors__{kws['vqgan_model']}_{fmt_perceptor_string(kws['+mmc_models'])}\"},\n",
    ")\n",
    "\n",
    "\n",
    "#exp_vqgan_base_perceptors.variant = variant\n",
    "# Also to add: \n",
    "# * other MMC perceptors\n",
    "# * more perceptor pairings\n",
    "# * perceptors vs. the other image models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "from loguru import logger\n",
    "from pittybook_utils import (\n",
    "    ExperimentMatrix\n",
    ")\n",
    "\n",
    "import re\n",
    "\n",
    "def get_perceptor_ids(in_str):\n",
    "    return re.findall(r\"id:'(.+?)'\", in_str)\n",
    "\n",
    "def fmt_perceptor_string(in_str):\n",
    "    return '_'.join(\n",
    "        [\n",
    "            p.replace('/','') \n",
    "            for p in get_perceptor_ids(in_str)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "exp_vqgan_perceptors_increased_resolution = ExperimentMatrix(\n",
    "# These need to be redone because they were blocked by errors\n",
    "variant = {\n",
    "        'vqgan_model':(\n",
    "            'imagenet',\n",
    "            'coco',\n",
    "            'wikiart',\n",
    "            'openimages',\n",
    "            'sflckr',\n",
    "        ),\n",
    "        '+mmc_models':(\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/32'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'ViT-B/16'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN50'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN101'}]\",\n",
    "            \"[{architecture:'clip',publisher:'openai',id:'RN50x4'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'RN50--openai'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'RN50--yfcc15m'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'RN50--cc12m'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'RN50-quickgelu--openai'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'RN50-quickgelu--yfcc15m'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'RN101--openai'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'RN101--yfcc15m'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'RN101--cc12m'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'RN101-quickgelu--openai'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'RN101-quickgelu--yfcc15m'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'RN50x4--openai'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32--openai'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32--laion400m_e31'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32--laion400m_e32'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32--laion400m_avg'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32-quickgelu--openai'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32-quickgelu--laion400m_e31'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32-quickgelu--laion400m_e32'}]\",\n",
    "            # \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-32-quickgelu--laion400m_avg'}]\",\n",
    "            \"[{architecture:'clip',publisher:'mlfoundations',id:'ViT-B-16--openai'}]\",\n",
    "        ),\n",
    "        #'reencode_each_frame':(True,False),\n",
    "        #'reset_lr_each_frame':(True,False)\n",
    "        #'direct_stabilization_weight':(0,0.3,1)\n",
    "        #'semantic_stabilization_weight':(0,0.3,1)\n",
    "},\n",
    "invariant = {\n",
    "        #'init_image':\"https://www.seattle.gov/images//images/Departments/ParksAndRecreation/Parks/GHI/GasWorksPark3.jpg\",\n",
    "        'init_image':\"/home/dmarx/proj/pytti-book/GasWorksPark3.jpg\",\n",
    "        'direct_stabilization_weight':0.3,\n",
    "        'cutouts':60,\n",
    "        'cut_pow':1,\n",
    "        #'reencode_each_frame':True,\n",
    "        #'reencode_each_frame':False,\n",
    "        #'reset_lr_each_frame':True,\n",
    "        'allow_overwrite':False,\n",
    "        'pixel_size':1,\n",
    "        'height':512,\n",
    "        'width':1024,\n",
    "        'scenes':'\"a photograph of a bright and beautiful spring day, by Trey Ratcliff || a painting of a cold wintery landscape, by Rembrandt \"',\n",
    "        'scene_suffix':'\" | text:-1:-.9 | watermark:-1:-.9\"',\n",
    "        'image_model':\"VQGAN\",\n",
    "        '+use_mmc':True,\n",
    "        'steps_per_frame':50,\n",
    "        'steps_per_scene':1000,\n",
    "        'interpolation_steps':500,\n",
    "        'animation_mode':\"2D\",\n",
    "        #'translate_y':-1,\n",
    "        'translate_x':-1,\n",
    "        'zoom_x_2d':3,\n",
    "        'zoom_y_2d':3,\n",
    "        'seed':12345,\n",
    "    },\n",
    "    # variable imputation doesn't seem to work in the overrides\n",
    "    mapped = {\n",
    "        'steps_per_frame':('pre_animation_steps', 'save_every'),\n",
    "        'steps_per_scene':('display_every',),\n",
    "    },\n",
    "    #conditional = {'gradient_accumulation_steps': lambda kws: 1 if kws['cutouts'] < 100 else 4}\n",
    "    conditional = {'file_namespace':\n",
    "        lambda kws: f\"exp_vqgan_perceptors_increased_resolution__{kws['vqgan_model']}_{fmt_perceptor_string(kws['+mmc_models'])}\"},\n",
    ")\n",
    "\n",
    "\n",
    "#exp_vqgan_base_perceptors.variant = variant\n",
    "# Also to add: \n",
    "# * other MMC perceptors\n",
    "# * more perceptor pairings\n",
    "# * perceptors vs. the other image models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "from loguru import logger\n",
    "from pittybook_utils import (\n",
    "    ExperimentMatrix\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_perceptor_ids(in_str):\n",
    "    return re.findall(r\"id:'(.+?)'\", in_str)\n",
    "\n",
    "def fmt_perceptor_string(in_str):\n",
    "    return '_'.join(\n",
    "        [\n",
    "            p.replace('/','') \n",
    "            for p in get_perceptor_ids(in_str)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "exp_stability_modes = ExperimentMatrix(\n",
    "    variant={\n",
    "        'reencode_each_frame':(True,False),\n",
    "        'reset_lr_each_frame':(True,False),\n",
    "        'direct_stabilization_weight':np.linspace(start=0,stop=1,num=3),\n",
    "        'semantic_stabilization_weight':np.linspace(start=0,stop=1,num=3),\n",
    "        'edge_stabilization_weight':np.linspace(start=0,stop=1,num=3),\n",
    "        'depth_stabilization_weight':np.linspace(start=0,stop=1,num=3),\n",
    "        #'direct_init_weight':np.linspace(start=0,stop=1,num=4),\n",
    "        #'semantic_init_weight':np.linspace(start=0,stop=1,num=4),\n",
    "    },\n",
    "    invariant = {\n",
    "        'vqgan_model':'sflckr',\n",
    "        #'ViT_B32':True # implied\n",
    "        'init_image':\"/home/dmarx/proj/pytti-book/GasWorksPark3.jpg\", # I think this really needs to be a video input experiment.\n",
    "        #'direct_stabilization_weight':0.3,\n",
    "        'cutouts':60,\n",
    "        'cut_pow':1,\n",
    "        #'reencode_each_frame':True,\n",
    "        #'reencode_each_frame':False,\n",
    "        #'reset_lr_each_frame':True,\n",
    "        'allow_overwrite':False,\n",
    "        'pixel_size':1,\n",
    "        'height':512,\n",
    "        'width':512,\n",
    "        #'scenes':'\"a photograph of a bright and beautiful spring day, by Trey Ratcliff || a painting of a cold wintery landscape, by Rembrandt \"',\n",
    "        'scenes':'\"a photograph of a bright and beautiful spring day, by Trey Ratcliff\"',\n",
    "        'scene_suffix':'\" | text:-1:-.9 | watermark:-1:-.9\"',\n",
    "        'image_model':\"VQGAN\",\n",
    "        #'+use_mmc':True,\n",
    "        'steps_per_frame':50,\n",
    "        'steps_per_scene':1000,\n",
    "        #'interpolation_steps':500,\n",
    "        'animation_mode':\"2D\",\n",
    "        #'translate_y':-1,\n",
    "        'translate_x':-1,\n",
    "        'zoom_x_2d':3,\n",
    "        'zoom_y_2d':3,\n",
    "        'seed':12345,\n",
    "    },\n",
    "    # variable imputation doesn't seem to work in the overrides\n",
    "    mapped = {\n",
    "        'steps_per_frame':('pre_animation_steps', 'save_every'),\n",
    "        'steps_per_scene':('display_every',),\n",
    "    },\n",
    "    #conditional = {'gradient_accumulation_steps': lambda kws: 1 if kws['cutouts'] < 100 else 4}\n",
    "    #conditional = {'file_namespace':\n",
    "    #    lambda kws: f\"exp_stability_modes_{kws['vqgan_model']}_{fmt_perceptor_string(kws['+mmc_models'])}\"},\n",
    "    conditional = {'file_namespace': \n",
    "                lambda kws: '_'.join(\n",
    "                    [\"exp_stability_modes\"]+[\n",
    "                        f\"{setting_name_shorthand(k)}-{v}\" for k,v in kws.items() if k in (\n",
    "                            'direct_stabilization_weight',\n",
    "                            'semantic_stabilization_weight',\n",
    "                            'edge_stabilization_weight',\n",
    "                            'depth_stabilization_weight',\n",
    "                            'direct_init_weight',\n",
    "                            'semantic_init_weight',\n",
    "                            'reencode_each_frame',\n",
    "                            'reset_lr_each_frame',\n",
    "                            )]\n",
    "                        )},\n",
    ")\n",
    "\n",
    "def setting_name_shorthand(setting_name):\n",
    "    return ''.join([tok[0] for tok in setting_name.split('_')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get some video mode shit up in here.\n",
    "\n",
    "from loguru import logger\n",
    "from pittybook_utils import (\n",
    "    ExperimentMatrix\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_perceptor_ids(in_str):\n",
    "    return re.findall(r\"id:'(.+?)'\", in_str)\n",
    "\n",
    "def fmt_perceptor_string(in_str):\n",
    "    return '_'.join(\n",
    "        [\n",
    "            p.replace('/','') \n",
    "            for p in get_perceptor_ids(in_str)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "exp_video_basic_stability_modes = ExperimentMatrix(\n",
    "    variant={\n",
    "        'reencode_each_frame':(True,False),\n",
    "        #'reset_lr_each_frame':(True,False),\n",
    "        'direct_stabilization_weight':np.linspace(start=0,stop=2,num=7),\n",
    "        'semantic_stabilization_weight':np.linspace(start=0,stop=2,num=7),\n",
    "        #'edge_stabilization_weight':np.linspace(start=0,stop=1,num=3),\n",
    "        #'depth_stabilization_weight':np.linspace(start=0,stop=1,num=3),\n",
    "        #'direct_init_weight':np.linspace(start=0,stop=1,num=4),\n",
    "        #'semantic_init_weight':np.linspace(start=0,stop=1,num=4),\n",
    "    },\n",
    "    invariant = {\n",
    "        'video_path':\"/home/dmarx/proj/pytti-book/pytti-core/src/pytti/assets/HebyMorgongava_512kb.mp4\",\n",
    "        'frames_per_second':15,\n",
    "        #'steps_per_frame':50,\n",
    "        #'steps_per_frame':80,\n",
    "        #'steps_per_scene':1000,\n",
    "        #'steps_per_scene':2000,\n",
    "        #'vqgan_model':'sflckr',\n",
    "        #'vqgan_model':'sflckr',\n",
    "        #'ViT_B32':True # implied\n",
    "        #'init_image':\"/home/dmarx/proj/pytti-book/GasWorksPark3.jpg\", # I think this really needs to be a video input experiment.\n",
    "        #'direct_stabilization_weight':0.3,\n",
    "        'cutouts':40,\n",
    "        'cut_pow':1,\n",
    "        #'reencode_each_frame':True,\n",
    "        #'reencode_each_frame':False,\n",
    "        #'reset_lr_each_frame':True,\n",
    "        'allow_overwrite':False,\n",
    "        'pixel_size':1,\n",
    "        'height':512,\n",
    "        'width':1024,\n",
    "        #'scenes':'\"a photograph of a bright and beautiful spring day, by Trey Ratcliff || a painting of a cold wintery landscape, by Rembrandt \"',\n",
    "        'scenes':'\"a photograph of a bright and beautiful spring day, by Trey Ratcliff\"',\n",
    "        'scene_suffix':'\" | text:-1:-.9 | watermark:-1:-.9\"',\n",
    "        'image_model':\"VQGAN\",\n",
    "        #'+use_mmc':True,\n",
    "        'steps_per_frame':50,\n",
    "        'steps_per_scene':1000,\n",
    "        #'interpolation_steps':500,\n",
    "        #'animation_mode':\"2D\",\n",
    "        'animation_mode':\"Video Source\",\n",
    "        #'translate_y':-1,\n",
    "        #'translate_x':-1,\n",
    "        #'zoom_x_2d':3,\n",
    "        #'zoom_y_2d':3,\n",
    "        'seed':12345,\n",
    "        'backups':3,\n",
    "    },\n",
    "    # variable imputation doesn't seem to work in the overrides\n",
    "    mapped = {\n",
    "        'steps_per_frame':('pre_animation_steps', 'save_every'),\n",
    "        'steps_per_scene':('display_every',),\n",
    "    },\n",
    "    #conditional = {'gradient_accumulation_steps': lambda kws: 1 if kws['cutouts'] < 100 else 4}\n",
    "    #conditional = {'file_namespace':\n",
    "    #    lambda kws: f\"exp_stability_modes_{kws['vqgan_model']}_{fmt_perceptor_string(kws['+mmc_models'])}\"},\n",
    "    conditional = {'file_namespace': \n",
    "                lambda kws: '_'.join(\n",
    "                    [\"exp_video_basic_stability_modes\"]+[\n",
    "                        f\"{setting_name_shorthand(k)}-{v}\" for k,v in kws.items() if k in (\n",
    "                            'direct_stabilization_weight',\n",
    "                            'semantic_stabilization_weight',\n",
    "                            #'edge_stabilization_weight',\n",
    "                            #'depth_stabilization_weight',\n",
    "                            #'direct_init_weight',\n",
    "                            #'semantic_init_weight',\n",
    "                            'reencode_each_frame',\n",
    "                            #'reset_lr_each_frame',\n",
    "                            )]\n",
    "                        )},\n",
    ")\n",
    "\n",
    "def setting_name_shorthand(setting_name):\n",
    "    return ''.join([tok[0] for tok in setting_name.split('_')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "%matplotlib inline\n",
    "#exp_limited_palette.run_all()\n",
    "#exp_vqgan_base_perceptors.run_all() # 281m\n",
    "#exp_vqgan_base_perceptors_2.run_all() # 32m\n",
    "#exp_vqgan_perceptors_increased_resolution.run_all() # later\n",
    "#exp_stability_modes.run_all()\n",
    "exp_video_basic_stability_modes.run_all()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3eff1e1332ed0784bebe5613522d192d113df675730803c3b8984f113f4e15fd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytti-book-l72HEyWC')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
